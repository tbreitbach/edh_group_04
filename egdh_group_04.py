# -*- coding: utf-8 -*-
"""Tim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BT_HdpTpyL9b0FvVcuNawTXL2JYJDwTn

**Swiss Electricity Demand**

**In short: **
*   Problem: Need to better understand the high-resolution energy data
*   Tool: Platform for model-based and historical data comparison using novel indicators
*   Result: More insight in ongoing energy transition and ability to compare electricity data

**What is it about?**

The Swiss energy system is transforming, but how? How did it change from last year to this year, from yesterday to today? How will the forecasted energy system for 2050 differ from todayâ€™s one? So far, we lack smart approaches to identify present and future changes in the energy system that go beyond aggregated indicators such as annual electricity demand changes or new power generation capacities installed.

Relevant links: https://nexus-e.org/nexus-e-participates-in-energy-hackdays-2021/

### *Importing essentials, data and preparing your data set*
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
import datetime as dt
import plotly.express as px
import statistics

pd.set_option('display.float_format', lambda x: '%.5f' % x)

drive.mount('/content/drive')

#Reading in the desired xls files from MyDrive. 
file_2009 = ('/content/drive/MyDrive/00_Energy Data/01_Historical_elec_dem_Swissgrid/EnergieUebersichtCH-2009.xls')
sheet='Zeitreihen0h15'
file_2019 = ('/content/drive/MyDrive/00_Energy Data/01_Historical_elec_dem_Swissgrid/EnergieUebersichtCH-2019-4.xls')
data_2009 = pd.read_excel(file_2009, sheet_name=sheet, index_col=0)
data_2019 = pd.read_excel(file_2019, sheet_name=sheet, index_col=0)

#Rename the desired column to consumption
#Please note that the choice of column (i.e. of specific energy consumption) impacts your analysis. Decide whether you want to look at the consumption including gen
data_2009.rename(columns = {'Summe verbrauchte Energie Regelblock Schweiz\nTotal energy consumption Swiss controlblock':'consumption'}, inplace = True)
data_2019.rename(columns = {'Summe verbrauchte Energie Regelblock Schweiz\nTotal energy consumption Swiss controlblock':'consumption'}, inplace = True)

#Here you drop all column except for thhe consumption column. 
data_2009.drop(data_2009.columns.difference(['consumption']), 1, inplace=True)
data_2019.drop(data_2019.columns.difference(['consumption']), 1, inplace=True)

#This drops the first row, as this row only includes units instead of load values. Please remember the units of the loads can be checked in this first row of the original data set. 
data_2009 = data_2009.iloc[1:]
data_2019 = data_2019.iloc[1:]

#This resets the index so the index really is an autoincremental integer instead of the datetime
data_2009.reset_index(inplace=True)
data_2009.rename(columns = {'index':'datetime'}, inplace = True)
data_2019.reset_index(inplace=True)
data_2019.rename(columns = {'index':'datetime'}, inplace = True)

#Merge the data sets of 2009 and 2019
data = pd.merge(data_2009, data_2019, left_index=True, right_index=True)

data.rename(columns = {'consumption_x':'consumption_2009', 'consumption_y':'consumption_2019', 'datetime_x':'datetime'}, inplace = True)

# Drop duplicate datetime after join
data.drop(columns=['datetime_y'])

#Save your dataframe to a csv file
data.to_csv('/content/drive/MyDrive/00_Energy Data/01_Historical_elec_dem_Swissgrid/consumption_2009_2019.csv')

# Formate everything right to datetime and add dayofweek feature
data['datetime_dt_2009']= pd.to_datetime(data['datetime'])
data['datetime_dow_2009']= data['datetime_dt_2009'].dt.dayofweek
data['datetime_dt_2019']= pd.to_datetime(data['datetime_y'])
data['datetime_dow_2019']= data['datetime_dt_2019'].dt.dayofweek

# Extract columns out of data dataframe
data_2019_shift = data[['datetime_y', 'consumption_2019', 'datetime_dt_2019', 'datetime_dow_2019']].copy()
data_2009_shift = data[['datetime', 'consumption_2009', 'datetime_dt_2009', 'datetime_dow_2009']].copy()

# Shift dates by 2 days to have the dataset monday on monday
data_2019_shifted = data_2019_shift.iloc[192:-1]
data_2009_shifted = data_2009_shift.iloc[:-193]

# resets the index to have the datetime as a seperate column
data_2019_shifted = data_2019_shifted.reset_index()

# merges the 2009 and 2019 dataframes on the integer index
data_shifted = pd.merge(data_2009_shifted, data_2019_shifted, left_index=True, right_index=True)

# exports the dataframe so that teammembers can start from this point on
data_shifted.to_csv('/content/drive/MyDrive/00_Energy Data/01_Historical_elec_dem_Swissgrid/consumption_shifted_2009_2019.csv')

# substracts consumptions based on benchmark which in this case is 2009
data_shifted["substraction_absolut"] = data_shifted['consumption_2009'] - data_shifted['consumption_2019']
data_shifted["substraction_relative"] = abs(data_shifted['substraction_absolut']) / data_shifted['consumption_2009']

# calculates the shift from one row to the row below
data_shifted["difference_2009"] = data_shifted.consumption_2009.diff()
data_shifted["difference_2019"] = data_shifted.consumption_2019.diff()

# extracts week of year, month of year, day of year and year and put it into data_shifted dataframe
data_shifted['datetime_2009_woy'] = data_shifted['datetime_dt_2009'].dt.isocalendar().week 
data_shifted['datetime_2009_moy'] = data_shifted['datetime_dt_2009'].dt.month
data_shifted['datetime_2009_doy'] = data_shifted['datetime_dt_2009'].dt.dayofyear
data_shifted['datetime_2009_yoy'] = data_shifted['datetime_dt_2009'].dt.year

# calculate the variance in the 2009 df grouped by week of year
data_shifted_var_woy = pd.DataFrame(data_shifted[['consumption_2009', 'datetime_2009_woy']])
data_shifted_var_woy['consumption_2009'] = pd.to_numeric(data_shifted_var_woy.consumption_2009)
var_woy = data_shifted_var_woy.groupby(by="datetime_2009_woy").var()

# calculate the variance in the 2009 df grouped by day of year
data_shifted_var_doy = pd.DataFrame(data_shifted[['consumption_2009', 'datetime_2009_doy']])
data_shifted_var_doy['consumption_2009'] = pd.to_numeric(data_shifted_var_doy.consumption_2009)
var_doy_2009 = data_shifted_var_doy.groupby(by="datetime_2009_doy").var()
var_doy_2009.rename(columns = {'consumption_2009':'consumption_variance_2009_doy'}, inplace = True)

# # calculate the variance in the 2009 df grouped by week of year
data_shifted_var_woy = pd.DataFrame(data_shifted[['consumption_2009', 'datetime_2009_woy']])
data_shifted_var_woy['consumption_2009'] = pd.to_numeric(data_shifted_var_woy.consumption_2009)
var_woy_2009 = data_shifted_var_woy.groupby(by="datetime_2009_woy").var()
var_woy_2009.rename(columns = {'consumption_2009':'consumption_variance_2009_woy'}, inplace = True)

# calculate the variance in the 2009 df grouped by month of year
data_shifted_var_moy = pd.DataFrame(data_shifted[['consumption_2009', 'datetime_2009_moy']])
data_shifted_var_moy['consumption_2009'] = pd.to_numeric(data_shifted_var_moy.consumption_2009)
var_moy_2009 = data_shifted_var_moy.groupby(by="datetime_2009_moy").var()
var_moy_2009.rename(columns = {'consumption_2009':'consumption_variance_2009_moy'}, inplace = True)

# # calculate the variance in the 2009 df grouped by year
data_shifted_var_yoy = pd.DataFrame(data_shifted[['consumption_2009', 'datetime_2009_yoy']])
data_shifted_var_yoy['consumption_2009'] = pd.to_numeric(data_shifted_var_yoy.consumption_2009)
var_yoy_2009 = data_shifted_var_yoy.groupby(by="datetime_2009_yoy").var()
var_moy_2009.rename(columns = {'consumption_2009':'consumption_variance_2009_yoy'}, inplace = True)

# merge the 2009 dataframe of variances together
data_shifted_var = pd.merge(data_shifted, var_doy_2009, on='datetime_2009_doy')
data_shifted_var_2 = pd.merge(data_shifted_var, var_woy_2009, on='datetime_2009_woy')
data_shifted_var_3 = pd.merge(data_shifted_var_2, var_moy_2009, on='datetime_2009_moy')
data_shifted_var_4 = pd.merge(data_shifted_var_3, var_yoy_2009, on='datetime_2009_yoy')
data_shifted_var_4.rename(columns = {'consumption_2009_x':'consumption_2009'}, inplace = True)

# change the object to a regular dataframe
data_shifted_var_2009 = pd.DataFrame(data_shifted_var_4[['consumption_2009', 'datetime_2009_yoy','consumption_variance_2009_doy', 'consumption_variance_2009_woy', 'consumption_variance_2009_moy', 'consumption_2009_y']])

# same thing 2019
data_shifted['datetime_2019_woy'] = data_shifted['datetime_dt_2019'].dt.isocalendar().week 
data_shifted['datetime_2019_moy'] = data_shifted['datetime_dt_2019'].dt.month
data_shifted['datetime_2019_doy'] = data_shifted['datetime_dt_2019'].dt.dayofyear
data_shifted['datetime_2019_yoy'] = data_shifted['datetime_dt_2019'].dt.year

data_shifted_var_woy = pd.DataFrame(data_shifted[['consumption_2019', 'datetime_2019_woy']])
data_shifted_var_woy['consumption_2019'] = pd.to_numeric(data_shifted_var_woy.consumption_2019)
var_woy_2019 = data_shifted_var_woy.groupby(by="datetime_2019_woy").var()

data_shifted_var_doy = pd.DataFrame(data_shifted[['consumption_2019', 'datetime_2019_doy']])
data_shifted_var_doy['consumption_2019'] = pd.to_numeric(data_shifted_var_doy.consumption_2019)
var_doy_2019 = data_shifted_var_doy.groupby(by="datetime_2019_doy").var()
var_doy_2019.rename(columns = {'consumption_2019':'consumption_variance_2019_doy'}, inplace = True)

data_shifted_var_woy = pd.DataFrame(data_shifted[['consumption_2019', 'datetime_2019_woy']])
data_shifted_var_woy['consumption_2019'] = pd.to_numeric(data_shifted_var_woy.consumption_2019)
var_woy_2019 = data_shifted_var_woy.groupby(by="datetime_2019_woy").var()
var_woy_2019.rename(columns = {'consumption_2019':'consumption_variance_2019_woy'}, inplace = True)

data_shifted_var_moy = pd.DataFrame(data_shifted[['consumption_2019', 'datetime_2019_moy']])
data_shifted_var_moy['consumption_2019'] = pd.to_numeric(data_shifted_var_moy.consumption_2019)
var_moy_2019 = data_shifted_var_moy.groupby(by="datetime_2019_moy").var()
var_moy_2019.rename(columns = {'consumption_2019':'consumption_variance_2019_moy'}, inplace = True)

data_shifted_var_yoy = pd.DataFrame(data_shifted[['consumption_2019', 'datetime_2019_yoy']])
data_shifted_var_yoy['consumption_2019'] = pd.to_numeric(data_shifted_var_yoy.consumption_2019)
var_yoy_2019 = data_shifted_var_yoy.groupby(by="datetime_2019_yoy").var()
var_moy_2019.rename(columns = {'consumption_2019':'consumption_variance_2019_yoy'}, inplace = True)

data_shifted_var = pd.merge(data_shifted, var_doy_2019, on='datetime_2019_doy')
data_shifted_var_2 = pd.merge(data_shifted_var, var_woy_2019, on='datetime_2019_woy')
data_shifted_var_3 = pd.merge(data_shifted_var_2, var_moy_2019, on='datetime_2019_moy')
data_shifted_var_4 = pd.merge(data_shifted_var_3, var_yoy_2019, on='datetime_2019_yoy')
data_shifted_var_4.rename(columns = {'consumption_2019_x':'consumption_2019'}, inplace = True)

data_shifted_var_4

data_shifted_var_2019 = pd.DataFrame(data_shifted_var_4[['consumption_2019', 'datetime_2019_yoy', 'datetime_2019_woy','datetime_2019_moy','consumption_variance_2019_doy', 'consumption_variance_2019_woy', 'consumption_variance_2019_moy', 'consumption_2019_y']])

#function to calculate the 95% value of the monthly average, for month number x

def extreme_values(period, number): #period can be monthly, weekly or yearly, number is the number of the desired week/day 
  if period == 'monthly':
    data = data_shifted_var_moy[data_shifted_var_moy['datetime_2009_moy'] == number] 
    quantile = data.quantile(q=0.95, axis=0, numeric_only=True, interpolation='linear')

    text = "This is the 95% quantile value of month number " + str(number)

    df[data.datatime_2009_moy > 10]

  if period == 'weekly':
    data = data_shifted_var_woy[data_shifted_var_woy['datetime_2009_woy'] == number] 
    quantile = data.quantile(q=0.95, axis=0, numeric_only=True, interpolation='linear')
    text = "This is the 95% quantile value of week number" + str(number)
  return quantile, text

data_shifted_var_2019['diff_doy_woy'] = abs(data_shifted_var_2019['consumption_variance_2019_doy'] - data_shifted_var_2019['consumption_variance_2019_woy'])

data_shifted_var_2019

# minimum of each week for the classification of typical day each week
data_shifted_var_2019.groupby('datetime_2019_woy')['diff_doy_woy'].min()

"""# Visuals"""

# plot 15 min values 2009 and 2019 
fig = px.line(data, y="consumption_2009")
fig.add_scatter(y=data['consumption_2019'], mode='lines', hovertext=data['consumption_2019'], hoverinfo="text",)
fig.show()

# group and aggregate the data by hour 
data_hourly = data.groupby([pd.Grouper(key='datetime', freq='h')])['consumption_2009', 'consumption_2019'].sum().reset_index().sort_values('datetime')

# plot by hour
fig = px.line(data_hourly, y="consumption_2009")
fig.add_scatter(y=data_hourly['consumption_2019'], mode='lines', hovertext=data_hourly['consumption_2019'], hoverinfo="text",)
fig.show()

data_daily = data.groupby([pd.Grouper(key='datetime', freq='D')])['consumption_2009', 'consumption_2019'].sum().reset_index().sort_values('datetime')

# plot everything daily frequency
fig = px.line(data_daily, y="consumption_2009")
fig.add_scatter(y=data_daily['consumption_2019'], mode='lines', hovertext=data_daily['consumption_2019'], hoverinfo="text",)
fig.show()

data_weekly = data.groupby([pd.Grouper(key='datetime', freq='W')])['consumption_2009', 'consumption_2019'].sum().reset_index().sort_values('datetime')

# plot everything weekly
fig = px.line(data_weekly, y="consumption_2009")
fig.add_scatter(y=data_weekly['consumption_2019'], mode='lines', hovertext=data_weekly['consumption_2019'], hoverinfo="text",)
fig.show()

data_monthly = data.groupby([pd.Grouper(key='datetime', freq='M')])['consumption_2009', 'consumption_2019'].sum().reset_index().sort_values('datetime')

# plot everything monthly
fig = px.line(data_monthly, y="consumption_2009")
fig.add_scatter(y=data_monthly['consumption_2019'], mode='lines', hovertext=data_monthly['consumption_2019'], hoverinfo="text",)
fig.show()

# violin plots for distribution in year 2009
fig = px.violin(data,  y="consumption_2009",box=True, hover_data=data.columns)
fig.show()

# violin plot for 2019
fig_2 = px.violin(data,  y="consumption_2019",box=True, hover_data=data.columns)
fig_2.show()

# plot after shifting
fig = px.line(data_shifted, y="consumption_2009")
fig.add_scatter(y=data_shifted['consumption_2019'], mode='lines', hovertext=data_shifted['consumption_2019'], hoverinfo="text")
fig.show()

# plot the differences between the two timeseries
fig = px.bar(data_shifted, y='substraction_absolut', color='datetime_dow_2009')
fig.show()

# plot substraction in relative numbers
fig = px.line(data_shifted, y='substraction_relative')
fig.show()

# plot the ramping differences
fig = px.line(data_shifted, y="difference_2009")
fig.add_scatter(y=data_shifted['difference_2019'], mode='lines', hovertext=data_shifted['difference_2019'], hoverinfo="text")
fig.show()

# plot of variances doy to woy of 2009
fig = px.line(data_shifted_var_2009, y="consumption_variance_2009_doy")
fig.add_scatter(y=data_shifted_var_2009['consumption_variance_2009_woy'], mode='lines', hovertext=data_shifted_var_2009['consumption_variance_2009_woy'], hoverinfo="text")
fig.show()

# plot of variance doy to moy
fig = px.line(data_shifted_var_2009, y="consumption_variance_2009_doy")
fig.add_scatter(y=data_shifted_var_2009['consumption_variance_2009_moy'], mode='lines', hovertext=data_shifted_var_2009['consumption_variance_2009_moy'], hoverinfo="text")
fig.show()

# Comparison of Variances
fig = px.line(data_shifted_var_2009, y="consumption_variance_2009_doy")
fig.add_scatter(y=data_shifted_var_2019['consumption_variance_2019_doy'], mode='lines', hovertext=data_shifted_var_2019['consumption_variance_2019_moy'], hoverinfo="text")
fig.show()

# plot of variances doy to woy
fig = px.line(data_shifted_var_2019, y="consumption_variance_2019_doy")
fig.add_scatter(y=data_shifted_var_2019['consumption_variance_2019_woy'], mode='lines', hovertext=data_shifted_var_2019['consumption_variance_2019_woy'], hoverinfo="text")
fig.show()